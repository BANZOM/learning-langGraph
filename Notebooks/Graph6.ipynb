{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1220c2",
   "metadata": {},
   "source": [
    "# Graph 6: Self Reflective RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse, BedrockEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "from typing import TypedDict, List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf74c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
    "    getpass(\"Enter LangSmith API Key: \")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\") or \\\n",
    "    \"default-project\"\n",
    "    \n",
    "print(\"LangSmith configured with project:\", os.environ[\"LANGCHAIN_PROJECT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client and the ChatBedrockConverse LLM\n",
    "\n",
    "def _build_bedrock_client():\n",
    "    return boto3.client(\n",
    "        \"bedrock-runtime\",\n",
    "        region_name=\"ap-south-1\",\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    )\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "        model=os.getenv(\"MODEL_NAME\", \"mistral.magistral-small-2509\"),\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "        region_name=\"ap-south-1\",\n",
    "        client=_build_bedrock_client(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d026317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it's a RAG, so we need to store the embeddings first. \n",
    "docs = (\n",
    "    PyPDFLoader(os.getenv(\"RAG_DOCS_PATH\")).load()\n",
    ")\n",
    "\n",
    "chunks = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=500,\n",
    ").split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the BedrockEmbeddings model\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=os.getenv(\"EMBEDDING_MODEL\", \"amazon.titan-embed-text-v2:0\"),\n",
    "    region_name=\"ap-south-1\",\n",
    "    client=_build_bedrock_client(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c183116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the vector store and adding the documents to it\n",
    "embeddings_dir = os.getenv(\"EMBEDDINGS_STORE_PATH\", \"./embeddings_store\")\n",
    "\n",
    "if not os.path.exists(embeddings_dir):\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore.save_local(embeddings_dir)\n",
    "else:\n",
    "    vectorstore = FAISS.load_local(embeddings_dir, embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4496978",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"Cryptographic keys & Algorithms\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cdf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent State Schema\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    need_retrieval: bool\n",
    "    retrieved_docs: List[Document]\n",
    "    relevant_docs: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e837c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Output for a Retrieve Decision Node\n",
    "class RetrieveDecisionSchema(BaseModel):\n",
    "    need_retrieval: bool = Field(\n",
    "        ...,\n",
    "        description=\"True if the agent needs to perform retrieval to answer the question reliably, False otherwise.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"A brief explanation of why the agent decided to retrieve or not.\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "# Structured Output for Relevancy Check for filtering retrieved docs\n",
    "class RelevancyCheckSchema(BaseModel):\n",
    "    is_relevant: bool = Field(\n",
    "        ...,\n",
    "        description=\"True if the document is relevant to the question, False otherwise.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"A brief explanation of why these documents are relevant and others are not.\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for the Retrieve Decision Node\n",
    "retrieve_decision_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are an intelligent agent designed to answer questions based on a given set of documents. Your task is to determine whether you need to perform retrieval from the document store to answer the question reliably. Guidelines to decide if retrieval is needed:\n",
    "        - True: If the question is specific and likely requires information that is not commonly known or is detailed in the documents.\n",
    "        - False: If the question is general and can be answered based on common knowledge or does not require specific information from the documents.\n",
    "        Respond with Valid JSON Only:\n",
    "        {json_schema}\n",
    "        \"\"\"),\n",
    "        (\"human\", \"<Question>{question}</Question>\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prompt for Direct Answer Generation Node\n",
    "direct_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are an intelligent agent designed to answer questions based on your knowledge. Your task is to generate a direct answer to the question based on your existing knowledge. Guidelines for generating the answer:\n",
    "        - Provide a concise and accurate answer to the question.\n",
    "        - Do not include any information that is not relevant to the question.\n",
    "        - If you are unsure about the answer, you can state that you do not know.\n",
    "        \"\"\"),\n",
    "        (\"human\", \"<Question>{question}</Question>\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prompt for Relevancy Check Node\n",
    "relevancy_check_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are an intelligent agent designed to filter retrieved documents based on their relevance to a given question. Your task is to determine whether the retrieved documents are relevant to the question. Guidelines for determining relevance:\n",
    "        - True: If the document contains information that is directly related to the question and can help in answering it.\n",
    "        - False: If the document does not contain relevant information or is not related to the question.\n",
    "        Respond with Valid JSON Only:\n",
    "        {json_schema}\n",
    "        \"\"\"),\n",
    "        (\"human\", \"<Question>{question}</Question><Retrieved_Docs>{retrieved_docs}</Retrieved_Docs>\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node: Retrieve Decision Node\n",
    "def retrieve_decision_node(agent_state: AgentState) -> AgentState:\n",
    "    ## If model is good and support structured output\n",
    "    # retrieve_decision_response = llm.with_structured_output(RetrieveDecisionSchema).invoke(\n",
    "    #     retrieve_decision_prompt.format_messages(question=agent_state[\"question\"], json_schema=parser.get_format_instructions())\n",
    "    # )\n",
    "    \n",
    "    ## If model is not good and does not support structured output\n",
    "    parser = PydanticOutputParser(pydantic_object=RetrieveDecisionSchema)\n",
    "    try:\n",
    "        retrieve_decision_response = llm.invoke(\n",
    "            retrieve_decision_prompt.format_messages(question=agent_state[\"question\"], json_schema=parser.get_format_instructions())\n",
    "        )\n",
    "        retrieve_decision_response = parser.parse(retrieve_decision_response.content)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing the response, defaulting to need_retrieval=True. Error:\", e)\n",
    "        retrieve_decision_response = RetrieveDecisionSchema(need_retrieval=True)\n",
    "    \n",
    "    return {\"need_retrieval\": retrieve_decision_response.need_retrieval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scratchpad for testing\n",
    "\n",
    "## here, i'm testing if the structured output parsing is working correctly or not. And it'll also help in understanding if model is capable of Tool Call.\n",
    "# check_retrieval_response = llm.with_structured_output(RetrieveDecisionSchema).invoke(\n",
    "#         retrieve_decision_prompt.format_messages(question=\"What are the CO's and POs of case 56\")\n",
    "#     )\n",
    "# print(check_retrieval_response)\n",
    "\n",
    "\n",
    "## Here, I'm testing smaller models who might not do tool call, but we can check if they are able to follow the instructions for structured output or not.\n",
    "# from langchain_core.output_parsers import PydanticOutputParser\n",
    "# parser = PydanticOutputParser(pydantic_object=RetrieveDecisionSchema)\n",
    "# response = llm.invoke(\n",
    "#     retrieve_decision_prompt.format_messages(question=\"Who is the team player in last month\") +\n",
    "#     [(\"system\", f\"Respond with valid JSON only:\\n{parser.get_format_instructions()}\")]\n",
    "# )\n",
    "# check_retrieval_response = parser.parse(response.content)\n",
    "# print(check_retrieval_response)\n",
    "# # print(parser.get_format_instructions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node : Generate Answer from LLM Data Node \n",
    "def direct_answer_node(agent_state: AgentState) -> AgentState:\n",
    "    direct_answer_response = llm.invoke(\n",
    "        direct_answer_prompt.format_messages(question=agent_state[\"question\"])\n",
    "    )\n",
    "    \n",
    "    return {\"answer\": direct_answer_response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c106bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node : Retrive Docs Node\n",
    "def retrieve_docs_node(agent_state: AgentState) -> AgentState:\n",
    "    retrieved_docs = retriever.invoke(agent_state[\"question\"])\n",
    "    return {\"retrieved_docs\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router : Router Function for deciding the flow based on the retrieval decision\n",
    "def router_node(agent_state: AgentState) -> Literal[\"direct_answer_node\", \"retrieve_docs_node\"]:\n",
    "    if agent_state[\"need_retrieval\"]:\n",
    "        return \"retrieve_docs_node\"\n",
    "    else:\n",
    "        return \"direct_answer_node\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node: Filter out non relevant docs out of all Retrieved Docs \n",
    "def filter_relevant_docs_node(agent_state: AgentState) -> AgentState:\n",
    "    relevant_docs: List[Document] = []\n",
    "    parser = PydanticOutputParser(pydantic_object=RelevancyCheckSchema)\n",
    "    \n",
    "    for doc in agent_state[\"retrieved_docs\"]:\n",
    "        relevancy_check_response = llm.invoke(\n",
    "            relevancy_check_prompt.format_messages(\n",
    "                question=agent_state[\"question\"],\n",
    "                retrieved_docs=doc.page_content,\n",
    "                json_schema=parser.get_format_instructions()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            relevancy_check_response = parser.parse(relevancy_check_response.content)\n",
    "            if relevancy_check_response.is_relevant:\n",
    "                relevant_docs.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing relevancy check response for doc {doc.metadata['source']}, defaulting to relevant. Error: {e}\")\n",
    "            relevant_docs.append(doc)\n",
    "\n",
    "    return {\"relevant_docs\": relevant_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80027285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: The above implementation of filter_relevant_docs_node is sequential and can be slow if there are many retrieved documents. We can optimize it by parallelizing the relevancy checks using ThreadPoolExecutor.\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# def check_relevancy(doc, question, parser):\n",
    "#     try:\n",
    "#         response = llm.invoke(\n",
    "#             relevancy_check_prompt.format_messages(\n",
    "#                 question=question,\n",
    "#                 retrieved_docs=doc.page_content,\n",
    "#                 json_schema=parser.get_format_instructions()\n",
    "#             )\n",
    "#         )\n",
    "#         result = parser.parse(response.content)\n",
    "#         return doc if result.is_relevant else None\n",
    "#     except:\n",
    "#         return doc  # Default to relevant on error\n",
    "\n",
    "\n",
    "# def filter_relevant_docs_node(agent_state: AgentState) -> AgentState:\n",
    "#     parser = PydanticOutputParser(pydantic_object=RelevancyCheckSchema)\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#         results = executor.map(\n",
    "#             lambda doc: check_relevancy(doc, agent_state[\"question\"], parser),\n",
    "#             agent_state[\"retrieved_docs\"]\n",
    "#         )\n",
    "\n",
    "#     relevant_docs = [doc for doc in results if doc is not None]\n",
    "#     return {\"relevant_docs\": relevant_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: If the number of retrieved documents is small (like 3-5), the sequential version might be simpler and sufficient. But if we expect a larger number of retrieved documents, the parallelized version can significantly reduce the time taken for relevancy checks.\n",
    "# def filter_relevant_docs_node(agent_state: AgentState) -> AgentState:\n",
    "#     if not agent_state[\"retrieved_docs\"]:\n",
    "#         return {\"relevant_docs\": []}\n",
    "\n",
    "#     # Combine all docs into one prompt\n",
    "#     docs_text = \"\\n\\n---\\n\\n\".join([\n",
    "#         f\"Document {i+1}:\\n{doc.page_content}\"\n",
    "#         for i, doc in enumerate(agent_state[\"retrieved_docs\"])\n",
    "#     ])\n",
    "\n",
    "#     batch_prompt = f\"\"\"Question: {agent_state[\"question\"]}\n",
    "\n",
    "# Documents:\n",
    "# {docs_text}\n",
    "\n",
    "# For each document, return JSON array with relevancy:\n",
    "# {{\"results\": [{{\"doc_index\": 0, \"is_relevant\": true}}, ...]}}\"\"\"\n",
    "\n",
    "#     response = llm.invoke(batch_prompt)\n",
    "\n",
    "#     try:\n",
    "#         result = json.loads(response.content)\n",
    "#         relevant_docs = [\n",
    "#             agent_state[\"retrieved_docs\"][r[\"doc_index\"]]\n",
    "#             for r in result[\"results\"]\n",
    "#             if r[\"is_relevant\"]\n",
    "#         ]\n",
    "#     except Exception as e:\n",
    "#         print(\"Error parsing batch relevancy response, defaulting to all docs relevant. Error:\", e)\n",
    "#         relevant_docs = agent_state[\"retrieved_docs\"]  # Fallback\n",
    "\n",
    "#     return {\"relevant_docs\": relevant_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4cebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# adding nodes\n",
    "graph.add_node(\"retrieve_decision_node\", retrieve_decision_node)\n",
    "graph.add_node(\"direct_answer_node\", direct_answer_node)\n",
    "graph.add_node(\"retrieve_docs_node\", retrieve_docs_node)\n",
    "graph.add_node(\"relevancy_check_node\", filter_relevant_docs_node)\n",
    "\n",
    "# making edges\n",
    "graph.add_edge(START, \"retrieve_decision_node\")\n",
    "graph.add_conditional_edges(\n",
    "    \"retrieve_decision_node\",\n",
    "    router_node,\n",
    "    {\n",
    "        \"direct_answer_node\": \"direct_answer_node\",\n",
    "        \"retrieve_docs_node\": \"retrieve_docs_node\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"direct_answer_node\", END)\n",
    "graph.add_edge(\"retrieve_docs_node\", \"relevancy_check_node\")\n",
    "graph.add_edge(\"relevancy_check_node\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# Run the visualization\n",
    "from IPython.display import display, Image\n",
    "display(Image(app.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_1 = app.invoke(\n",
    "    {\n",
    "        \"question\": \"What are the cryptographic algorithms ?\",\n",
    "        \"need_retrieval\": False,\n",
    "        \"retrieved_docs\": [],\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Case 1 Output:\\n\", json.dumps(test_case_1, indent=2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_2 = app.invoke(\n",
    "    {\n",
    "        \"question\": \"Who is the managing the infosec team\",\n",
    "        \"need_retrieval\": False,\n",
    "        \"retrieved_docs\": [],\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Test Case 2 Output:\\n\", test_case_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_case_2[\"relevant_docs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400fdff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langgraph (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
